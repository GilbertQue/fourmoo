{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Gilbert's Documentation","text":"<p>This site is automatically updated when Word documents are uploaded to Discord!</p>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>Drop a <code>.docx</code> file in the <code>#docs-to-publish</code> Discord channel</li> <li>The automation converts it to Markdown</li> <li>It gets pushed here automatically</li> <li>GitHub Pages publishes it</li> </ol> <p>Last updated: Auto-generated</p>"},{"location":"spark-vs-python-vs-warehouse-microsoft-fabric/","title":"Spark vs Python vs Warehouse - Which One to Use in Microsoft Fabric","text":"<p>Spark vs Python vs Warehouse Comparing time and Capacity Usage in Microsoft Fabric</p> <p>After my previous blog post on the different semantic model options and at the same time working with a Fabric customer, it got me thinking which is faster and which consumes less capacity when ingesting data into Power BI either via the SQL Endpoint to a Lakehouse or a query from the Warehouse.</p> <p>Below you will find the information which I found very interesting indeed.</p>"},{"location":"spark-vs-python-vs-warehouse-microsoft-fabric/#loading-data-from-csv","title":"Loading data from CSV","text":"<p>The first step was to load the data from a CSV file into the Lakehouse and Warehouse</p> <p>Here is the code that I used to load the data into Lakehouse</p> <p></p> <p>Shown below is how long it took along with the CUs</p> <p></p> <p>Next, I was to load the same data into the Warehouse, below is the code that I used</p> <p></p> <p>Shown below is how long this took</p> <p></p> <p>A quick comparison</p> <p>Item Type</p> <p>Duration</p> <p>CUs</p> <p>Lakehouse</p> <p>588</p> <p>14,464</p> <p>Warehouse</p> <p>88</p> <p>1,923</p> <p>As shown above the Warehouse ran 500 seconds faster or over 6.6x (660% quicker).</p> <p>When looking at the Capacity Units comparison the Warehouse used 12,541 less capacity units or consumed 7.5x (750% less capacity units)</p>"},{"location":"spark-vs-python-vs-warehouse-microsoft-fabric/#transforming-from-staging-into-aggregated-table","title":"Transforming from Staging into Aggregated table","text":"<p>Next, what I did was to use the staging table to create an aggregated table (something typically done when moving from Bronze to Silver)</p> <p>Below is the Spark SQL Code that I ran</p> <p></p> <p>Shown below is how long it took along with the CUs</p> <p></p> <p>I then ran the same query on the Warehouse inserting into a Warehouse table.</p> <p></p> <p>Shown below is how long it took along with the CUs</p> <p></p> <p>A quick comparison for loading the aggregated data.</p> <p>Item Type</p> <p>Duration</p> <p>CUs</p> <p>Lakehouse</p> <p>255</p> <p>2,818</p> <p>Warehouse</p> <p>55</p> <p>1,727</p> <p>As shown above the Warehouse ran 200 seconds faster or over 4.6x (460% quicker).</p> <p>When looking at the Capacity Units comparison the Warehouse used 1,901 less capacity units or consumed 1.6x (160% less capacity units)</p>"},{"location":"spark-vs-python-vs-warehouse-microsoft-fabric/#query-comparison-of-lh-vs-wh-and-wh-with-clustering","title":"Query Comparison of LH vs WH and WH with Clustering","text":"<p>The final comparison I wanted to compare was to load from Lakehouse, Warehouse and Warehouse Clustered tables into my Semantic Model using Import Mode.</p> <p>It did take some time to run, because it was importing 100 million rows.</p> <p>All the details below used the default navigation of finding the table using Power Query with the SQL Server connection.</p> <p>Lakehouse query below of CUs</p> <p></p> <p>Warehouse Query</p> <p></p> <p>Warehouse Query on Clustered table</p> <p></p> <p>A quick comparison for loading the aggregated data.</p> <p>Item Type</p> <p>Duration</p> <p>CUs</p> <p>Lakehouse</p> <p>2,826</p> <p>126,697</p> <p>Warehouse</p> <p>1,434</p> <p>48,098</p> <p>Warehouse Clustered Table</p> <p>1,519</p> <p>52,875</p> <p>As shown above the Warehouse ran 1,329 seconds faster or over 1.97x (197% quicker).</p> <p>When looking at the Capacity Units comparison the Warehouse used 78,599 less capacity units or consumed 2.6x (260% less capacity units)</p> <p>While the Warehouse Clustered table ran faster than the Lakehouse the Warehouse was still the quickest.</p>"},{"location":"spark-vs-python-vs-warehouse-microsoft-fabric/#summary","title":"Summary","text":"<p>When comparing the ingestion of the CSV file, creating an aggregated table and then importing the table into the semantic model the clear winner is the Warehouse as shown with the overall numbers below.</p> <p>Process</p> <p>Lakehouse Duration</p> <p>Warehouse Duration</p> <p>% Diff (LH vs WH)</p> <p>Loading CSV</p> <p>588</p> <p>88</p> <p>668%</p> <p>Table Aggregation</p> <p>255</p> <p>55</p> <p>464%</p> <p>Query into Semantic Model</p> <p>2,826</p> <p>1,434</p> <p>197%</p> <p>TOTALS</p> <p>3,669</p> <p>1,577</p> <p>233%</p> <p>Process</p> <p>Lakehouse CUs</p> <p>Warehouse CUs</p> <p>% Diff (LH vs WH)</p> <p>Loading CSV</p> <p>14,464</p> <p>1,923</p> <p>752%</p> <p>Table Aggregation</p> <p>2,818</p> <p>1,727</p> <p>163%</p> <p>Query into Semantic Model</p> <p>126,697</p> <p>48,098</p> <p>263%</p> <p>TOTALS</p> <p>143,979</p> <p>51,748</p> <p>278%</p> <p>Thanks for reading I hope you found this insightful!</p>"}]}